# 
# TRAINING_SCRIPT= bash_scripts/run_train_e2d2_fineweb_sky.sh \
# sky exec --num-nodes 2 -c dllm sky/train_e2d2_fineweb.sky.yaml --env TRAINING_SCRIPT
workdir: .
envs:
    - TRAINING_SCRIPT: null
run: |
  # https://docs.mosaicml.com/projects/composer/en/stable/notes/distributed_training.html#multi-node-arguments
  # experiments:
  # - one node (gpu_1x_a100_sxm4), no dist vars: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r3
  # - one node (gpu_1x_a100_sxm4), w/ dist vars: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r4
  # - two nodes (2x gpu_1x_a100_sxm4): https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r6
  # - two nodes (2x gpu_8x_a100), DDP: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r10
  # - one node (gpu_8x_a100_80gb_sxm4): https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r11
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=4, prefetch=2, workers=2: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r13
  #    - batch=4 means MICRO_BATCH_SIZE=4 in the run_train_e2d2_fineweb_sky.sh script
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=4, prefetch=0, workers=0: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r14
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=8, prefetch=2, workers=2: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r15
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=16, prefetch=2, workers=2: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r16
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=16, prefetch=2, workers=2, swap fsdp degrees: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r18
  #    - Lowers throughput by 40% with data_parallel_shard_degree=$NUM_NODES (which should be wrong) but also lowers ethernet traffic by %40 (?)
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=16, prefetch=2, workers=2, hybrid_shard: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r19
  #    - Lowers throughput by 22%
  # - two nodes (2x gpu_8x_a100_80gb_sxm4), batch=16, prefetch=2, workers=2, no fsdp: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r20
  #    - Increased throughput over r16 by 43% (best so far)
  #    - Oddly, ethernet traffic maxes at 2.8Gbit/s vs ~8Gbit/s for r16
  # - one node (1x gpu_8x_a100_80gb_sxm4), batch=16, prefetch=2, workers=2, no fsdp: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r21
  #    - OOM -- the batch size logic in the e2d2 script is wrong because composer takes nodes into account
  #    - The microbatch size must have doubled (check that)
  # - one node (1x gpu_8x_a100_80gb_sxm4), batch=8, prefetch=2, workers=2, no fsdp: https://wandb.ai/eric-czech/discrete-diffusion/runs/fineweb_block8_lr3e-4_bsz512_warm2000ba_max-dur1000000ba_enc20_dec4_hidden512_inter2048_e2d2_r22
  #    - Same as r21 but with batch=8

  set -exo pipefail

  TAG=e2d2_r22
  # GPU=a100_sxm4
  # GPU=h100_sxm5
  GPU=a100_80gb_sxm4

  MICRO_BATCH_SIZE=16 # w/ 2 nodes
  MICRO_BATCH_SIZE=8  # w/ 1 node

  NUM_NODES=$SKYPILOT_NUM_NODES
  NODE_RANK=$SKYPILOT_NODE_RANK
  MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  MASTER_PORT=12345
  LOCAL_WORLD_SIZE=$(nvidia-smi -L | wc -l)
  WORLD_SIZE=$(( NUM_NODES * LOCAL_WORLD_SIZE ))
  # Set NODENAME for composer logging at:
  # https://github.com/mosaicml/composer/blob/v0.32.1/composer/trainer/trainer.py#L1481
  NODENAME="sky-node-${NODE_RANK}"

  required_vars=("LOCAL_WORLD_SIZE" "WORLD_SIZE" "NUM_NODES" "NODE_RANK" "MASTER_ADDR" "MASTER_PORT")
  for var in "${required_vars[@]}"; do
      if [ -z "${!var}" ]; then
          echo "Error: $var not set."
          exit 1
      fi
  done

  # Use pre-computed TorchInductor cache, if possible
  bash bash_scripts/init_torch_cache.sh $GPU

  # Notes:
  # - Do not forget to set --network host or processes hang at "Initializing dist"; see:
  #   https://github.com/mosaicml/composer/blob/v0.32.1/composer/cli/launcher.py#L304
  # - /dev/shm size defaults to 64M which is far too low for tensor sharing between
  #   processes and causes errors like:
  #   RuntimeError: unable to write to file </torch_3819_4141600215_6>: No space left on device (28)
  # - On caching of Triton/TorchInductor auto-tuning: 
  #   - https://github.com/triton-lang/triton/issues/6355
  #   - https://docs.pytorch.org/tutorials/recipes/torch_compile_caching_configuration_tutorial.html
  #     - "Note that if TRITON_CACHE_DIR is not set in the environment, Inductor sets the Triton cache directory"
  sudo docker run \
    --env-file ~/.env \
    -e RUN_DIR=/workspace/outputs \
    -e DATA_DIR=/workspace/data \
    -e NUM_NODES=$NUM_NODES \
    -e WORLD_SIZE=$WORLD_SIZE \
    -e NODE_RANK=$NODE_RANK \
    -e MASTER_ADDR=$MASTER_ADDR \
    -e MASTER_PORT=$MASTER_PORT \
    -e NODENAME=$NODENAME \
    -e HF_HOME=/workspace/.hf_cache \
    -e TORCHINDUCTOR_CACHE_DIR=/workspace/.ti_cache \
    -e TORCHINDUCTOR_AUTOGRAD_CACHE=1 \
    -e TORCHINDUCTOR_FX_GRAPH_CACHE=1 \
    -e PYTHONPATH=/workspace:/workspace/.hf_cache/modules \
    -e TAG=$TAG \
    --network host \
    --shm-size=16g \
    --gpus all --rm -v $(pwd):/workspace \
    eczech/dllm-dev:py311-cuda128-torch270 \
    bash $TRAINING_SCRIPT