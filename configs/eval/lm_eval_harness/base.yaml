#file: noinspection LongLine
_target_: lm_eval.evaluator.simple_evaluate
tasks: ???
num_fewshot: 0
limit: null # Override with CLI for testing, e.g., `task.limit=10`
batch_size: 1
log_samples: true
system_instruction: null
apply_chat_template: false
random_seed: ${seed}
numpy_random_seed: ${.random_seed}
torch_random_seed: ${.random_seed}
fewshot_random_seed: ${.random_seed}
model:
  _target_: scripts.eval.harness_eval.LMEvalHarnessModel
  tokenizer: ${tokenizer}
  pretrained_model_name_or_path: ${pretrained_model_name_or_path}
  pretrained_model_revision: ${pretrained_model_revision}
  generated_samples_output_path: ${generated_samples_output_path}
  load_ema_weights: false
  ckpt_file: 'best-rank0.pt'  # best-rank0.pt or latest-rank0.pt
  gen_kwargs: null
